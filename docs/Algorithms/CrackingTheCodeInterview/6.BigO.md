# Big O

This is such an important concept that you should master it!

Big O time is the language and metric we use to describe the efficiency of algorithms. Not understanding it thoroughly can really hurt you in developing an algorithm. You will be judged harshly for not really understanding Big O, but you will also struggle to judge when your algorithm is getting faster or slower.

Master this concept!

## Time Complexity

This is what the concept of asymptotic runtime, or big O time, means.

> Asymptotic complexity is the key to comparing algorithms. Comparing absolute time is not particularly meaningful, because they are specific to particular hardware. Asymptotic complexity reveals deeper mathematical truths about algorithms that are independent of hardware.

We could describe the data transfer algorithm runtime as:

* Electronic Transfer: O(s), where s is the size of the file. This means that the time to transfer the file increases linearly with the size of the file.
* Airplane Transfer: O(1) with respect to the size of the file. As the size of the file increases, it won't take any longer to get the file to your friend. The time is constant.

No matter how big the constant is and how slow the linear increase is, linear will at some point surpass constant.

|
|O(1)     /
|_ _ _ _ /_ _ _ _ _ _ _ _ _
|       /
|      /
|     /
|    /O(s)
|   /
|  /
| /
|/
|___________________________

There are many more runtimes than this. Some of the most common ones are O(log N), O(N log N), O(N), O(N^2) and O(2^N). There's no fixed list of possible runtimes, though.

You can also have multiple variables in your runtime. For example, the time to paint a fence that's W meters wide and h meters high could be described as O(wh). If you needed p layers of paint, then you could say that the time is O(whp).

### Big O, Big Theta, and Big Omega

Academics use big O, big Theta, and big Omega to describe runtimes.

* O (big O): In academia, big O describes an upper bound on the time. An algorithm that prints all the values in an array could be described as O(N), but it could also be described as O(N^2), O(N^3), O(2^N) (or many other big O times). The algorithm is at least as fast as each of these; therefore they are upper bounds on the runtime. This is similar to a less-than-or-equal-to relationship.
* Big Omega: In academia, big omega is the equivalent concept but for lower bound. Printing the values in an array is Big Omega(N) as well as Big Omega(log N) and Big Omega(1). After all, you know that it won't be faster than those runtimes.
* Big Theta: In academia, Big Theta means both Big O and Big Omega. That is, an algorithm is Big Theta(N) if it is both O(N) and Big Omega(N). Bit Theta gives a tight bound on runtime.

In industry (and therefore in interviews), people seem to have merged Big Theta and Big O together. Industry's meaning of Big O is closer to what academics mean by Big Theta, in that it would be seen as incorrect to describe printing an array as O(N^2). Industry would just say this is O(N).

Let's try to think of Big O in the way that industry tends to use it: by always trying to offer the tightest description of the runtime.

### Best Case, Worst Case, and Expected Case