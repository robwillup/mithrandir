# What is Big O Notation?

Big-O notation is the language we use for talking about how
long an algorithm takes to run (time complexity) or how much
memory is used by an algorithm (space complexity). Big-O
notation can express the best, worst, and average-case
running time of an algorithm.

As a software engineer, you'll find that most discussions
of Big-O focus on the "upper-bound" running time of an
algorithm, which is often termed the worst-case. Big-O
notation does not directly equate to time a we know it
(e.g., seconds, milliseconds, microseconds, etc.). Analysis
of running time does not take certain things into account,
such as the processor, the language, or the run time
environment. Instead, we can think of time as the number of
operations or step it take to complete a problem of size *n*.
In other word, Big-O notation is a way to track how quickly
the runtime grows relative to the size of the input.

## Big O - Perspective 2

What is Big O notation?
Represents the efficiency of an algorithm.

Two types of Big O:

* Time complexity | performance
* Space complexity | memory use

