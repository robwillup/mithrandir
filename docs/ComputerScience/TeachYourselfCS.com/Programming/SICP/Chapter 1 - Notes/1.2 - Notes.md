# 1.2 Procedures and the Processes They Generate

After learning the elements of programming, such as primitive
arithmetic operations, combining operations, abstracting these
operations by defining them as compound procedure, one cannot
yet say that he knows how to program. At that stage, one
would still be lacking the knowledge of common patterns of
usage, which procedures are worth defining, the experience to
predict the consequences of making a move, like executing a
procedure.

> The ability to visualize the consequences of the actions
> under consideration is crucial to becoming an expert
> programmer.

**Rob's note: Therefore, understanding Big O notation is fundamental**

To become experts, we must learn to visualize the processes
generated by various types of procedures. Only after we have
developed such a skill can we learn to reliably construct
programs that exhibit the desired behavior.

A procedure is a pattern for the *local evolution* of a
computational process. It specifies how each stage of the
process is built upon the previous stage. We would like to be
able to make statements about the overall, or global,
behavior of a process whose local evolution has been
specified by a procedure. This is very difficult to do in
general, but we can at least try to describe some typical
patterns of process evolution.

Let's examine the "shapes" of processes generated by simple
procedures and also investigate the rates at which these
processes consume the important computational resources of
space and time.

## 1.2.1 Linear Recursion and Iteration

```scheme
(factorial 6)
(* 6 (factorial 5))
(* 6 (* 5 (factorial 4)))
(* 6 (* 5 (* 4 (factorial 3))))
(* 6 (* 5 (* 4 (* 3 (factorial 2)))))
(* 6 (* 5 (* 4 (* 3 (* 2 (factorial 1))))))
(* 6 (* 5 (* 4 (* 3 (* 2 1)))))
(* 6 (* 5 (* 4 (* 3 2 ))))
(* 6 (* 5 (* 4 6)))
(* 6 (* 5 24))
(* 6 120)
720
```

*A linear recursive process for computing 6!.*

We begin by considering the factorial function, defined by

```code
n! = n * (n - 1) * (n - 2) ... 3 . 2 . 1
```

There are many ways to compute factorial. One way is to make
use of the observation that n! is equal to n times (n -1)!
for any positive integer n:

```code
n! = n * [(n - 1) * (n - 2)...3 . 2 . 1] = n * (n - 1)!
```

Thus, we can compute n! by computing (n - 1)! and multiplying
the result by n. If we add the stipulation that 1! is equal
to 1, this observation translates directly into a procedure:

```scheme
(define (factorial n)
    (if (= n 1)
        1
        (* n (factorial (- n 1)))))
```

Now let's take a different perspective on computing
factorials. We could describe a rule for computing n! by
specifying that we first multiply 1 by 2, then multiply the
result by 3, and so on until we reach n. More formally, we
maintain a running product, together with a counter that
counts from 1 up to n. We can describe the computation by
saying that the counter and the product simultaneously change
from one step to the next according to the rule

product <== counter * product

counter <== counter + 1

and stipulating that n! is the value of the product when the
counter exceeds n.

```scheme
(factorial 6)
(fact-iter   1 1 6)
(fact-iter   1 2 6)
(fact-iter   2 3 6)
(fact-iter   6 4 6)
(fact-iter  24 5 6)
(fact-iter 120 6 6)
(fact-iter 720 7 6)
720
```

*A linear iterative process for computing 6!*.\

Once again, we can recast our description as a procedure for
computing factorials:

```scheme
(define (factorial n)
    (fact-iter 1 1 n))

(define (fact-iter product counter max-count)
    (if (> counter max-count)
        product
        (fact-iter (* counter product)
                   (+ counter 1)
                   max-count)))
```

Compare the two processes. From one point of view, they seem
hardly different at all. Both compute the same mathematical function on the same
domain, and each requires a number of steps proportional to n to compute n!.
Indeed, both processes even carry out the same sequence of multiplication,
obtaining the same sequence of partial products. On the other hand, when we
consider the "shapes" of the two processes, we find that they evolve quite
differently.

Consider the first process. The substitution model reveals a shape of expansion
followed by
contraction. The expansion occurs as the process builds up a chain of deferred
operations. The contraction occurs as the operations are actually performed.
This type of process, characterized by a chain of deferred operations, is
called a *recursive process*. Carrying out this process requires that the
interpreter keeps track of the operations to be performed later on.
In the computation of n!, the length of the chain of deferred
multiplications, and hence the amount of information needed to keep track of
it, grows linearly with n (is proportional to n), just like the number of
steps. Such a process is called a *linear recursive process*.

By contrast, the second process does not grow and shrink. At each step, all we
need to keep track of, for any n, are the current values of the variables
`product`, `counter`, and `max-count`. We call this an `iterative process`.
In general, an iterative process is one whose state can be summarized by
a fixed number of `state variables`, together with a fixed rule that
describes how the state variables should be updated as the process moves
from state to state and an (optional) end test that specifies conditions
under which the process should terminate. In computing n! the number of
steps required grows linearly with n. Such process is called
a *linear iterative process*.

The contrast between the two processes can be seen in another way. In the
iterative case, the program variables provide a complete description of the
state of the process at any point. If we stopped the computation between
steps, all we would need to do to resume the computation is to supply the
interpreter with the values of the three program variables. Not so with the
recursive process. In this case there is some additional "hidden" information,
maintained by the interpreter and not contained in the program variables,
which indicates "where the process is" in negotiating the chain of deferred
operations. The longer the chain, the more information must be maintained.

In contrasting iteration and recursion, we must be careful
not to confuse the notion of a recursive *process* with the notion of a
recursive *procedure*. When we describe a procedure as recursive, we are referring
to the fact that the procedure definition refers (either directly or indirectly)
to the procedure itself. But when we describe a process as following a pattern that
is, say, linearly recursive, we are speaking about how the process evolves, not
about the syntax of how a procedure is written. It may seem disturbing that we
refer to a recursive procedure such as `fact-iter` as generating an
iterative process. However, the process really is iterative: Its state is captured
completely by its three state variables, and an interpreter need keep track of
only three variables in order to execute the process.

One reason that the distinction between process and procedure may be confusing
is that most implementations of common languages (including Ada, Pascal, and C)
are designed in such a way that the interpretation of any recursive procedure
consumes an amount of memory that grows with the number of procedure calls, even
when the process described is, in principle, iterative. As a consequence, these
languages can describe iterative processes only be resorting to special-purpose
"looping constructs" such as `do, repeat, until, for,` and `while`.
The implementation of Scheme we shall consider in chapter 5 does not share this defect.
It will execute an iterative process in constant space, even if the iterative process
is described by a recursive procedure. An implementation with this property is called
*tail-recursive*. With a tail-recursive implementation, iteration can be expressed
using the ordinary procedure call mechanism, so that special iteration constructions
are useful only as syntactic sugar.
